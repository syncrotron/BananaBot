{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "tensorflow",
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Banana Bot\n",
    "## By Samuel Horovatin"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, datetime\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# General Network Constants\n",
    "EPOCHS = 100\n",
    "BATCH = 32\n",
    "OPTIMIZER = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Data Imports Constants \n",
    "DATA_PATH = '.\\\\data\\\\bananas\\\\'\n",
    "LABEL_PATH = \"Estu.csv\"\n",
    "VALIDATION_SPLIT = 0.5\n",
    "SEED = 1996\n",
    "HEIGHT_SHIFT = 0.10\n",
    "WIDTH_SHIFT = 0.10\n",
    "ZOOM_RANGE  = 0.10 \n",
    "K_FOLDS = 10\n",
    "EFF_LVL = 'B0'\n",
    "\n",
    "# Network Construction Constants\n",
    "INPUT_SHAPE = {'B0':(224,224,3), 'B1':(240,240,3), 'B2':(260,260,3), 'B3':(300,300,3), 'B4':(380,380,3), 'B5':(456,456,3), 'B6':(528,528,3), 'B7':(600,600,3)} # outlined in keras docs for efficentnet implementation https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning/\n",
    "CONV_DEPTH = 256 # Taken from https://github.com/potterhsu/easy-fpn.pytorch. Not 100% sure why it is 256...\n",
    "TARGET_SIZE = INPUT_SHAPE[EFF_LVL][:2]\n",
    "\n",
    "def MRD(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true,'float32')\n",
    "    # y_pred = y_pred[:, 0] # Uncomment if utilizing Uncertainty_Loss()\n",
    "    return tf.keras.backend.sum(tf.keras.backend.abs(y_pred - y_true) / y_true )\n",
    "\n",
    "def One_FVU(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true,'float32')\n",
    "    # y_pred = y_pred[:, 0] # Uncomment if utilizing Uncertainty_Loss()\n",
    "    y_mean = tf.keras.backend.mean(y_true)\n",
    "    num = tf.math.squared_difference(y_true, y_pred)\n",
    "    den = tf.keras.backend.sum(y_true - y_mean)\n",
    "    return 1 - ( num / den )\n",
    "\n",
    "# As described by Kendall and Gal: https://proceedings.neurips.cc/paper/2017/hash/2650d6089a6d640c5e85b2b88265dc2b-Abstract.html\n",
    "# For regression, output_D should always be 1\n",
    "def Uncertainty_Loss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true,'float32')\n",
    "    mu = y_pred[:, 0]\n",
    "    si = y_pred[:, 1]\n",
    "    loss = (si + tf.math.squared_difference(y_true, mu) / tf.math.exp(si)) * 0.5\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "METRICS = [MRD, One_FVU]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets labels from label csv\n",
    "data_label_df = pd.read_csv(LABEL_PATH)\n",
    "\n",
    "# Create training and testing datasets\n",
    "dat_gen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255, \n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    height_shift_range=HEIGHT_SHIFT,\n",
    "    width_shift_range=WIDTH_SHIFT,\n",
    "    zoom_range=ZOOM_RANGE,\n",
    "    fill_mode=\"wrap\"\n",
    ")\n",
    "\n",
    "train_datagen = dat_gen.flow_from_dataframe(\n",
    "    dataframe=data_label_df,\n",
    "    directory=DATA_PATH,\n",
    "    x_col='FileName',\n",
    "    y_col='BananaCount',\n",
    "    target_size=TARGET_SIZE,\n",
    "    batch_size=BATCH,\n",
    "    class_mode='raw',\n",
    "    shuffle=True,\n",
    "    seed=SEED,\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "valid_datagen = dat_gen.flow_from_dataframe(\n",
    "    dataframe=data_label_df,\n",
    "    directory=DATA_PATH,\n",
    "    x_col='FileName',\n",
    "    y_col='BananaCount',\n",
    "    target_size=TARGET_SIZE,\n",
    "    batch_size=BATCH,\n",
    "    class_mode='raw',\n",
    "    shuffle=True,\n",
    "    seed=SEED,\n",
    "    subset=\"validation\"\n",
    ")\n",
    "\n",
    "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_datagen = test_datagen.flow_from_dataframe(\n",
    "    dataframe=data_label_df,\n",
    "    directory=DATA_PATH,\n",
    "    x_col='FileName',\n",
    "    y_col='BananaCount',\n",
    "    target_size=TARGET_SIZE,\n",
    "    batch_size=BATCH,\n",
    "    class_mode='raw',\n",
    "    shuffle=True,\n",
    "    seed=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizes images\n",
    "for _ in range(5):\n",
    "    img, label = train_datagen.next()\n",
    "    print(f\"SHAPE: {img.shape}, LABEL: {label[0]}\")   # (1,256,256,3)\n",
    "    plt.imshow(img[0])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains the model given the constant parameters and returns the model and the fit history\n",
    "def Train_Model(model, train_datagen, test_datagen, valid_datagen, loss_func):\n",
    "  model.compile(optimizer=OPTIMIZER,\n",
    "                loss=loss_func,\n",
    "                metrics=METRICS,\n",
    "                run_eagerly=True # For debugging\n",
    "                )\n",
    "\n",
    "  csv_logger = tf.keras.callbacks.CSVLogger('training.log', separator=',')\n",
    "  # early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.03, mode='min', patience=8)\n",
    "\n",
    "  STEP_SIZE_TRAIN=train_datagen.n//train_datagen.batch_size\n",
    "  STEP_SIZE_VALID=valid_datagen.n//valid_datagen.batch_size\n",
    "  STEP_SIZE_TEST=test_datagen.n//test_datagen.batch_size\n",
    "  earlystop = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "\n",
    "  history = model.fit(x=train_datagen,\n",
    "                      steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                      validation_data=valid_datagen,\n",
    "                      validation_steps=STEP_SIZE_VALID, \n",
    "                      epochs=EPOCHS,\n",
    "                      callbacks=[csv_logger])\n",
    "        \n",
    "  return (model, history)\n",
    "\n",
    "# Final consolidation layer of FPN predictions\n",
    "class WeightedAverage(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(WeightedAverage, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, model_outputs):\n",
    "        model_outputs = tf.convert_to_tensor(model_outputs)\n",
    "        estimates = model_outputs[:, 0]\n",
    "        variances = model_outputs[:, 1]\n",
    "        \n",
    "        weighted_avg = tf.cast(tf.math.reduce_sum(tf.math.divide(estimates, variances)),'float32')\n",
    "\n",
    "        # based on the wikipedia page for weighted variance average\n",
    "        avg_variance = tf.cast(tf.math.reduce_mean(variances),'float32')\n",
    "        return avg_variance * weighted_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes in an model and returns a list of edges last layers name in a block\n",
    "# Layers within blocks need to be named with '*block#*' were # is the block number\n",
    "def blockEdgeNameFinder(model):\n",
    "    layer_index = 0 # This allows the function to skip the start of the network\n",
    "    current_block = \"block\"\n",
    "    block_edges = []\n",
    "    last_layer_name = \"\"\n",
    "\n",
    "    for layer in model.layers:\n",
    "        if current_block in layer.name:\n",
    "            if layer_index == 0:\n",
    "                layer_index = layer_index + 1\n",
    "                current_block = f\"block{layer_index}\"\n",
    "                \n",
    "        elif layer_index == 0:\n",
    "            continue\n",
    "\n",
    "        elif \"block\" in last_layer_name:\n",
    "            layer_index = layer_index + 1\n",
    "            next_block = f\"block{layer_index}\"\n",
    "            current_block = next_block\n",
    "            block_edges.append(last_layer_name)\n",
    "        last_layer_name = layer.name\n",
    "\n",
    "    return block_edges\n",
    "\n",
    "# Takes in an model (which is a layer) and a name for the topper and adds the regression topper as described by the paper\n",
    "def Add_Regression_Top(model, name):\n",
    "    options = {\n",
    "        'activation'         : 'relu',\n",
    "        'kernel_initializer' : tf.keras.initializers.HeNormal()\n",
    "    }\n",
    "    outputs = tf.keras.layers.Conv2D(\n",
    "        256,\n",
    "        model.shape[1:3],\n",
    "        activation = options['activation'],\n",
    "        kernel_initializer= options['kernel_initializer'], \n",
    "        name=f'{name}_conv1_256'\n",
    "    )(model)\n",
    "    outputs = tf.keras.layers.Conv2D(\n",
    "        256,\n",
    "        outputs.shape[1:3],\n",
    "        activation = options['activation'],\n",
    "        kernel_initializer= options['kernel_initializer'], \n",
    "        name=f'{name}_conv2_256'\n",
    "    )(outputs)\n",
    "\n",
    "    outputs = tf.keras.layers.GlobalAveragePooling2D()(outputs)\n",
    "    outputs = tf.keras.layers.Dense(\n",
    "        256,\n",
    "        activation = options['activation'],\n",
    "        kernel_initializer= options['kernel_initializer'], \n",
    "        name=f'{name}_dense_256'\n",
    "    )(outputs)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(\n",
    "        128,\n",
    "        activation = options['activation'],\n",
    "        kernel_initializer= options['kernel_initializer'], \n",
    "        name=f'{name}_dense_128'\n",
    "    )(outputs)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(\n",
    "        64,\n",
    "        activation = options['activation'],\n",
    "        kernel_initializer= options['kernel_initializer'], \n",
    "        name=f'{name}_dense_64'\n",
    "    )(outputs)\n",
    "\n",
    "    # Uncomment if utilizing Uncertainty_Loss() and remove 1 wide dense layer output layer\n",
    "    # outputs = tf.keras.layers.Dense(\n",
    "    #     2, \n",
    "    #     kernel_initializer= options['kernel_initializer'],\n",
    "    #     name=f'{name}_out'\n",
    "    # )(outputs) \n",
    "\n",
    "    outputs = tf.keras.layers.Dense(\n",
    "        1, \n",
    "        kernel_initializer= options['kernel_initializer'],\n",
    "        name=f'{name}_out'\n",
    "    )(outputs)\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def EfficentNet(B_lvl=0):\n",
    "\n",
    "    if B_lvl == 'B0' or B_lvl == 0:\n",
    "        basemodel = tf.keras.applications.EfficientNetB0(input_shape=INPUT_SHAPE['B0'], include_top=False, weights='imagenet')\n",
    "    elif B_lvl == 'B1' or B_lvl == 1:\n",
    "        basemodel = tf.keras.applications.EfficientNetB1(input_shape=INPUT_SHAPE['B1'], include_top=False, weights='imagenet')\n",
    "    elif B_lvl == 'B2' or B_lvl == 2:\n",
    "        basemodel = tf.keras.applications.EfficientNetB2(input_shape=INPUT_SHAPE['B2'], include_top=False, weights='imagenet')\n",
    "    elif B_lvl == 'B3' or B_lvl == 3:\n",
    "        basemodel = tf.keras.applications.EfficientNetB3(input_shape=INPUT_SHAPE['B3'], include_top=False, weights='imagenet')\n",
    "    elif B_lvl == 'B4' or B_lvl == 4:\n",
    "        basemodel = tf.keras.applications.EfficientNetB4(input_shape=INPUT_SHAPE['B4'], include_top=False, weights='imagenet')\n",
    "    elif B_lvl == 'B5' or B_lvl == 5:\n",
    "        basemodel = tf.keras.applications.EfficientNetB5(input_shape=INPUT_SHAPE['B5'], include_top=False, weights='imagenet')\n",
    "    elif B_lvl == 'B6' or B_lvl == 6:\n",
    "        basemodel = tf.keras.applications.EfficientNetB6(input_shape=INPUT_SHAPE['B6'], include_top=False, weights='imagenet')\n",
    "    elif B_lvl == 'B7' or B_lvl == 7:\n",
    "        basemodel = tf.keras.applications.EfficientNetB7(input_shape=INPUT_SHAPE['B7'], include_top=False, weights='imagenet')   \n",
    "    else:\n",
    "        print(f\"EfficentNet Error: Unknown efficentnet level {B_lvl}, using EfficientNetB0 as defualt\")\n",
    "        basemodel = tf.keras.applications.EfficientNetB0(input_shape=INPUT_SHAPE['B0'], include_top=False, weights='imagenet')\n",
    "    return build_EfficentNet(basemodel)\n",
    "\n",
    "def build_EfficentNet(basemodel):\n",
    "    FPN_LAYER_NAMES = blockEdgeNameFinder(basemodel)\n",
    "    backbone = []\n",
    "\n",
    "    FPN_LAYER_NAMES = FPN_LAYER_NAMES[2:]\n",
    "    FPN_LAYER_NAMES = FPN_LAYER_NAMES[::-1] \n",
    "\n",
    "    for name in FPN_LAYER_NAMES: \n",
    "        backbone.append(basemodel.get_layer(name).output)\n",
    "\n",
    "    prediction_outputs = [] \n",
    "    first_layer = True\n",
    "    layer_index = 7\n",
    "    \n",
    "    for layer in backbone: \n",
    "        # FPN consolidation\n",
    "        FPN_layer = tf.keras.layers.Conv2D(filters=CONV_DEPTH, kernel_size=(1,1), name=f\"fpn{layer_index}_conv\")(layer) \n",
    "        if not first_layer:\n",
    "            last_shape = last_FPN_layer.shape[1:][1]\n",
    "            current_shape = FPN_layer.shape[1:][1] # All layers should be square resolution\n",
    "            scale = int(round(current_shape/last_shape))\n",
    "            if scale != 1:\n",
    "                last_FPN_layer = tf.keras.layers.UpSampling2D((scale,scale), name=f\"fpn{layer_index}_scale{scale}\")(last_FPN_layer)\n",
    "            \n",
    "            # Fixes non-int scaling between layers via zero padding\n",
    "            if current_shape % last_shape != 0:\n",
    "                scale = int(abs(FPN_layer.shape[1:][1] - last_FPN_layer.shape[1:][1]))\n",
    "                print(f\"FPN_layer.shape {FPN_layer.shape[1:][1]}, last_FPN_layer.shape {last_FPN_layer.shape[1:][1]}, scale {scale}\")\n",
    "                last_FPN_layer = tf.keras.layers.ZeroPadding2D(padding=scale, name=f\"fpn{layer_index}_zero_pad{scale}\")(last_FPN_layer)\n",
    "                print(f\"FPN_layer.shape {FPN_layer.shape[1:][1]}, last_FPN_layer.shape {last_FPN_layer.shape[1:][1]}, scale {scale}\")\n",
    "            \n",
    "        else:\n",
    "            first_layer = False\n",
    "\n",
    "        last_FPN_layer = FPN_layer\n",
    "        new_prediction = Add_Regression_Top(FPN_layer, f\"fpn_reg{layer_index}\")\n",
    "        prediction_outputs.append(new_prediction)\n",
    "\n",
    "        layer_index = layer_index - 1\n",
    "    final_model = tf.keras.models.Model(inputs=basemodel.inputs, outputs=prediction_outputs, name=\"EfficentNet_FPN\")\n",
    "    return final_model\n",
    "    \n",
    "rebuilt_model = EfficentNet(EFF_LVL)\n",
    "rebuilt_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = tf.keras.losses.log_cosh #somewhat padantic to declare here, but fixes strange auto type resolution bug\n",
    "(model, history) = Train_Model(rebuilt_model, train_datagen, test_datagen, valid_datagen, loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(valid_datagen, batch_size=BATCH)"
   ]
  }
 ]
}