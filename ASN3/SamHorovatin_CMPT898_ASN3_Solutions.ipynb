{
 "cells": [
  {
   "source": [
    "# CMPT 898: Assigment 3 Solutions\n",
    "## By Samuel Horovatin, sch923, 11185403"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline network: *You can start with a LeNet-5 style architecture that we discussed in the lecture. As a baseline start with ReLU activations for the hidden layers, and a softmax output layer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os, datetime\n",
    "import math\n",
    "\n",
    "EPOCHS = 10\n",
    "OPTIMIZER = 'adam'\n",
    "LOSS = 'sparse_categorical_crossentropy'\n",
    "METRICS = 'accuracy'\n",
    "\n",
    "# Grab the Cifar10 dataset, which is a color image database consisiting of \n",
    "# 10 different classes representing airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks\n",
    "(x_train, y_train),(x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "#Normalization of x_train and x_test and split into training dataset and testing dataset\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "def train_model(model):\n",
    "  model.compile(optimizer=OPTIMIZER,\n",
    "                loss=LOSS,\n",
    "                metrics=[METRICS])\n",
    "\n",
    "  logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "  tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "  model.fit(x=x_train, \n",
    "            y=y_train, \n",
    "            epochs=EPOCHS, \n",
    "            validation_data=(x_test, y_test),\n",
    "            callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 8s 167us/sample - loss: 1.6437 - accuracy: 0.3956 - val_loss: 1.4109 - val_accuracy: 0.4861\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 8s 150us/sample - loss: 1.3618 - accuracy: 0.5053 - val_loss: 1.3497 - val_accuracy: 0.5154\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 7s 144us/sample - loss: 1.2542 - accuracy: 0.5503 - val_loss: 1.2773 - val_accuracy: 0.5404\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 7s 142us/sample - loss: 1.1837 - accuracy: 0.5769 - val_loss: 1.2524 - val_accuracy: 0.5595\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 7s 144us/sample - loss: 1.1242 - accuracy: 0.6002 - val_loss: 1.1768 - val_accuracy: 0.5815\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 7s 146us/sample - loss: 1.0720 - accuracy: 0.6159 - val_loss: 1.2636 - val_accuracy: 0.5580\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 7s 144us/sample - loss: 1.0255 - accuracy: 0.6357 - val_loss: 1.2944 - val_accuracy: 0.5516\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 7s 143us/sample - loss: 0.9835 - accuracy: 0.6514 - val_loss: 1.1859 - val_accuracy: 0.5885\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 7s 148us/sample - loss: 0.9401 - accuracy: 0.6657 - val_loss: 1.2291 - val_accuracy: 0.5883\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 8s 156us/sample - loss: 0.9103 - accuracy: 0.6768 - val_loss: 1.1628 - val_accuracy: 0.5974\n",
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Reusing TensorBoard on port 6006 (pid 3752), started 3:10:04 ago. (Use '!kill 3752' to kill it.)"
     },
     "metadata": {}
    }
   ],
   "source": [
    "def create_baseline_model():\n",
    "  return tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters=6, \n",
    "        kernel_size=5, \n",
    "        activation='relu', \n",
    "        input_shape=(32, 32, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters=16,\n",
    "        kernel_size=5, \n",
    "        activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(120, activation='relu'),\n",
    "    tf.keras.layers.Dense(84, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')])\n",
    "\n",
    "%reload_ext tensorboard\n",
    "train_model(create_baseline_model())\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "source": [
    "### Add L2 weight decay regularization: *Add an L2-norm penalty on the weights of your baseline model as regularization. Test two different regularization strengths.*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training model 1 with L2 lambda of 0.001\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 8s 160us/sample - loss: 1.7692 - accuracy: 0.3969 - val_loss: 1.5944 - val_accuracy: 0.4550\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 7s 146us/sample - loss: 1.5288 - accuracy: 0.4872 - val_loss: 1.5007 - val_accuracy: 0.4927\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 7s 141us/sample - loss: 1.4581 - accuracy: 0.5175 - val_loss: 1.4213 - val_accuracy: 0.5308\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 7s 139us/sample - loss: 1.4097 - accuracy: 0.5384 - val_loss: 1.3934 - val_accuracy: 0.5446\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 7s 139us/sample - loss: 1.3793 - accuracy: 0.5496 - val_loss: 1.3885 - val_accuracy: 0.5470\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 7s 140us/sample - loss: 1.3553 - accuracy: 0.5604 - val_loss: 1.4000 - val_accuracy: 0.5486\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 7s 141us/sample - loss: 1.3327 - accuracy: 0.5723 - val_loss: 1.3504 - val_accuracy: 0.5638\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 7s 139us/sample - loss: 1.3186 - accuracy: 0.5805 - val_loss: 1.3743 - val_accuracy: 0.5616\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 7s 143us/sample - loss: 1.3028 - accuracy: 0.5883 - val_loss: 1.3532 - val_accuracy: 0.5662\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 7s 147us/sample - loss: 1.2852 - accuracy: 0.5964 - val_loss: 1.3536 - val_accuracy: 0.5709\n",
      "Training model 2 with L2 lambda of 0.01\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 8s 167us/sample - loss: 2.1872 - accuracy: 0.2713 - val_loss: 1.9244 - val_accuracy: 0.3580\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 7s 144us/sample - loss: 1.9253 - accuracy: 0.3560 - val_loss: 1.8661 - val_accuracy: 0.3859\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 7s 141us/sample - loss: 1.8717 - accuracy: 0.3880 - val_loss: 1.8278 - val_accuracy: 0.3972\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 7s 139us/sample - loss: 1.8369 - accuracy: 0.4115 - val_loss: 1.8583 - val_accuracy: 0.4043\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 7s 139us/sample - loss: 1.8150 - accuracy: 0.4263 - val_loss: 1.8082 - val_accuracy: 0.4269\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 7s 140us/sample - loss: 1.7991 - accuracy: 0.4335 - val_loss: 1.7997 - val_accuracy: 0.4402\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 7s 139us/sample - loss: 1.7869 - accuracy: 0.4396 - val_loss: 1.7856 - val_accuracy: 0.4377\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 7s 140us/sample - loss: 1.7765 - accuracy: 0.4452 - val_loss: 1.9207 - val_accuracy: 0.3981\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 7s 140us/sample - loss: 1.7680 - accuracy: 0.4481 - val_loss: 1.7516 - val_accuracy: 0.4483\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 7s 139us/sample - loss: 1.7641 - accuracy: 0.4492 - val_loss: 1.7459 - val_accuracy: 0.4589\n",
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Reusing TensorBoard on port 6006 (pid 3752), started 2:47:46 ago. (Use '!kill 3752' to kill it.)"
     },
     "metadata": {}
    }
   ],
   "source": [
    "#A model that applies an L2 regularization at every layer\n",
    "def create_L2_model(reg_strength):\n",
    "  return tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters=6, \n",
    "        kernel_size=5, \n",
    "        activation='relu', \n",
    "        input_shape=(32, 32, 3), \n",
    "        kernel_regularizer=tf.keras.regularizers.l2(reg_strength)),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters=16,\n",
    "        kernel_size=5, \n",
    "        activation='relu',\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(reg_strength)),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(\n",
    "        120,\n",
    "        activation='relu',\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(reg_strength)),\n",
    "    tf.keras.layers.Dense(\n",
    "        84,\n",
    "        activation='relu',\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(reg_strength)),\n",
    "    tf.keras.layers.Dense(10,\n",
    "        activation='softmax',\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(reg_strength))])\n",
    "\n",
    "print(f\"Training model 1 with L2 lambda of {LAMBDA1}\")\n",
    "train_model(create_L2_model(LAMBDA1))\n",
    "print(f\"Training model 2 with L2 lambda of {LAMBDA2}\")\n",
    "train_model(create_L2_model(LAMBDA2))\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "source": [
    "### Add L1 weight decay regularization: *Add an L1-norm penalty on the weights of your baseline model as regularization. Test two different regularization strengths.*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training model 1 with L1 lambda of 0.001\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 9s 170us/sample - loss: 2.2857 - accuracy: 0.2321 - val_loss: 2.0554 - val_accuracy: 0.2817\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 8s 157us/sample - loss: 1.9845 - accuracy: 0.3136 - val_loss: 1.8881 - val_accuracy: 0.3622\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 7s 150us/sample - loss: 1.8758 - accuracy: 0.3690 - val_loss: 1.8463 - val_accuracy: 0.3826\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 8s 151us/sample - loss: 1.8250 - accuracy: 0.3890 - val_loss: 1.7887 - val_accuracy: 0.3993\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 8s 152us/sample - loss: 1.7947 - accuracy: 0.4014 - val_loss: 1.7807 - val_accuracy: 0.4092\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 7s 143us/sample - loss: 1.7716 - accuracy: 0.4111 - val_loss: 1.7998 - val_accuracy: 0.4030\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 7s 149us/sample - loss: 1.7491 - accuracy: 0.4216 - val_loss: 1.7249 - val_accuracy: 0.4269\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 7s 148us/sample - loss: 1.7283 - accuracy: 0.4294 - val_loss: 1.7148 - val_accuracy: 0.4288\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 7s 148us/sample - loss: 1.7162 - accuracy: 0.4330 - val_loss: 1.6860 - val_accuracy: 0.4423\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 7s 150us/sample - loss: 1.7049 - accuracy: 0.4384 - val_loss: 1.6961 - val_accuracy: 0.4364\n",
      "Training model 2 with L1 lambda of 0.01\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 9s 171us/sample - loss: 3.4462 - accuracy: 0.0995 - val_loss: 2.3797 - val_accuracy: 0.1000\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 8s 165us/sample - loss: 2.3799 - accuracy: 0.0988 - val_loss: 2.3797 - val_accuracy: 0.1000\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 8s 164us/sample - loss: 2.3799 - accuracy: 0.0984 - val_loss: 2.3796 - val_accuracy: 0.1000\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 7s 145us/sample - loss: 2.3799 - accuracy: 0.0981 - val_loss: 2.3796 - val_accuracy: 0.1000\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 7s 143us/sample - loss: 2.3798 - accuracy: 0.0987 - val_loss: 2.3796 - val_accuracy: 0.1000\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 7s 144us/sample - loss: 2.3799 - accuracy: 0.0981 - val_loss: 2.3797 - val_accuracy: 0.1000\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 7s 146us/sample - loss: 2.3798 - accuracy: 0.0982 - val_loss: 2.3795 - val_accuracy: 0.1000\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 8s 153us/sample - loss: 2.3798 - accuracy: 0.0967 - val_loss: 2.3795 - val_accuracy: 0.1000\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 7s 146us/sample - loss: 2.3798 - accuracy: 0.0999 - val_loss: 2.3797 - val_accuracy: 0.1000\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 7s 145us/sample - loss: 2.3798 - accuracy: 0.0980 - val_loss: 2.3797 - val_accuracy: 0.1000\n",
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Reusing TensorBoard on port 6006 (pid 3752), started 3:13:23 ago. (Use '!kill 3752' to kill it.)"
     },
     "metadata": {}
    }
   ],
   "source": [
    "#A model that applies an L1 regularization at every layer\n",
    "def create_L1_model(reg_strength):\n",
    "  return tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters=6, \n",
    "        kernel_size=5, \n",
    "        activation='relu', \n",
    "        input_shape=(32, 32, 3), \n",
    "        kernel_regularizer=tf.keras.regularizers.l1(reg_strength)),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters=16,\n",
    "        kernel_size=5, \n",
    "        activation='relu',\n",
    "        kernel_regularizer=tf.keras.regularizers.l1(reg_strength)),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(\n",
    "        120,\n",
    "        activation='relu',\n",
    "        kernel_regularizer=tf.keras.regularizers.l1(reg_strength)),\n",
    "    tf.keras.layers.Dense(\n",
    "        84,\n",
    "        activation='relu',\n",
    "        kernel_regularizer=tf.keras.regularizers.l1(reg_strength)),\n",
    "    tf.keras.layers.Dense(10,\n",
    "        activation='softmax',\n",
    "        kernel_regularizer=tf.keras.regularizers.l1(reg_strength))])\n",
    "\n",
    "LAMBDA1 = 0.001\n",
    "LAMBDA2 = 0.01\n",
    "\n",
    "print(f\"Training model 1 with L1 lambda of {LAMBDA1}\")\n",
    "train_model(create_L1_model(LAMBDA1))\n",
    "print(f\"Training model 2 with L1 lambda of {LAMBDA2}\")\n",
    "train_model(create_L1_model(LAMBDA2))\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Remove fully-connected layers: *Modify the architecture to remove the fully-connected layers at the backend of the network. For example, replace with Global Average Pooling or an alternative. Report the change in the number of parameters for this model compared to previous.*\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training model with Global Average Pooling\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 8s 157us/sample - loss: 1.9423 - accuracy: 0.2819 - val_loss: 1.8484 - val_accuracy: 0.3020\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 7s 139us/sample - loss: 1.9514 - accuracy: 0.2907 - val_loss: 1.8270 - val_accuracy: 0.3148\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 7s 139us/sample - loss: 1.8738 - accuracy: 0.3102 - val_loss: 1.7695 - val_accuracy: 0.3250\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 7s 138us/sample - loss: 1.7964 - accuracy: 0.3412 - val_loss: 1.7924 - val_accuracy: 0.3460\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 7s 138us/sample - loss: 1.8042 - accuracy: 0.3404 - val_loss: 1.7683 - val_accuracy: 0.3529\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 7s 139us/sample - loss: 1.7645 - accuracy: 0.3574 - val_loss: 1.8819 - val_accuracy: 0.2992\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 7s 138us/sample - loss: 1.7812 - accuracy: 0.3508 - val_loss: 1.7524 - val_accuracy: 0.3700\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 7s 138us/sample - loss: 1.7509 - accuracy: 0.3678 - val_loss: 1.7741 - val_accuracy: 0.3516\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 7s 139us/sample - loss: 1.7190 - accuracy: 0.3850 - val_loss: 2.0659 - val_accuracy: 0.1848\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 7s 138us/sample - loss: 1.7701 - accuracy: 0.3477 - val_loss: 1.6895 - val_accuracy: 0.3890\n",
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Reusing TensorBoard on port 6006 (pid 3752), started 3:20:17 ago. (Use '!kill 3752' to kill it.)"
     },
     "metadata": {}
    }
   ],
   "source": [
    "#A model that applies an L1 regularization at every layer\n",
    "def create_average_pooling_model():\n",
    "  return tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters=6, \n",
    "        kernel_size=5, \n",
    "        activation='relu', \n",
    "        input_shape=(32, 32, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters=16,\n",
    "        kernel_size=5, \n",
    "        activation='relu'),\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Flatten()])\n",
    "\n",
    "print(f\"Training model with Global Average Pooling\")\n",
    "train_model(create_average_pooling_model())\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "source": [
    "### Analyze the accuracy of the different models: *For all six models, train/test your model three times to get a sense of the consistency of the test error. Keep other aspects of your model the same among designs (# epochs, mini-batch size, hyperparameters). Generate a table that summarizes the training error, test error, standard deviation of test error across three runs, inference time, and \\# of parameters for each model.*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "### Analyze the weights of the regularized models: *For the baseline model and the four regularized models (from parts 2 and 3: L2 and L1 regularization with two different strengths each) measure the sparsity of the weights in each FC layer and create a bar chart that compares the sparsity between the models in each layer. There are a number of metrics that measure sparsity, e.g. Hoyer's index. For di\u000berent sparsity metrics, see Table I in https://arxiv.org/abs/0811.4706*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}