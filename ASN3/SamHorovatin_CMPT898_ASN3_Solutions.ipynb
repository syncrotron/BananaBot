{
 "cells": [
  {
   "source": [
    "# CMPT 898: Assigment 3 Solutions\n",
    "## By Samuel Horovatin, sch923, 11185403"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline network: *You can start with a LeNet-5 style architecture that we discussed in the lecture. As a baseline start with ReLU activations for the hidden layers, and a softmax output layer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os, datetime\n",
    "import math\n",
    "\n",
    "EPOCHS = 16\n",
    "OPTIMIZER = 'adam'\n",
    "LOSS = 'sparse_categorical_crossentropy'\n",
    "METRICS = 'accuracy'\n",
    "\n",
    "# Grab the Cifar10 dataset, which is a color image database consisiting of \n",
    "# 10 different classes representing airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks\n",
    "(x_train, y_train),(x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "#Normalization of x_train and x_test and split into training dataset and testing dataset\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/16\n",
      "50000/50000 [==============================] - 12s 240us/sample - loss: 1.6082 - accuracy: 0.4104 - val_loss: 1.4358 - val_accuracy: 0.4699\n",
      "Epoch 2/16\n",
      "50000/50000 [==============================] - 8s 154us/sample - loss: 1.3382 - accuracy: 0.5184 - val_loss: 1.2950 - val_accuracy: 0.5311\n",
      "Epoch 3/16\n",
      "50000/50000 [==============================] - 7s 146us/sample - loss: 1.2355 - accuracy: 0.5574 - val_loss: 1.2252 - val_accuracy: 0.5561\n",
      "Epoch 4/16\n",
      "50000/50000 [==============================] - 7s 147us/sample - loss: 1.1574 - accuracy: 0.5877 - val_loss: 1.1804 - val_accuracy: 0.5805\n",
      "Epoch 5/16\n",
      "50000/50000 [==============================] - 7s 146us/sample - loss: 1.0868 - accuracy: 0.6149 - val_loss: 1.2010 - val_accuracy: 0.5825\n",
      "Epoch 6/16\n",
      "50000/50000 [==============================] - 7s 149us/sample - loss: 1.0348 - accuracy: 0.6333 - val_loss: 1.1624 - val_accuracy: 0.5962\n",
      "Epoch 7/16\n",
      "50000/50000 [==============================] - 7s 145us/sample - loss: 0.9805 - accuracy: 0.6556 - val_loss: 1.1563 - val_accuracy: 0.6013\n",
      "Epoch 8/16\n",
      "50000/50000 [==============================] - 7s 143us/sample - loss: 0.9343 - accuracy: 0.6706 - val_loss: 1.1556 - val_accuracy: 0.5973\n",
      "Epoch 9/16\n",
      "50000/50000 [==============================] - 7s 149us/sample - loss: 0.8978 - accuracy: 0.6820 - val_loss: 1.1454 - val_accuracy: 0.6040\n",
      "Epoch 10/16\n",
      "50000/50000 [==============================] - 7s 147us/sample - loss: 0.8633 - accuracy: 0.6952 - val_loss: 1.1571 - val_accuracy: 0.6046\n",
      "Epoch 11/16\n",
      "50000/50000 [==============================] - 7s 144us/sample - loss: 0.8238 - accuracy: 0.7085 - val_loss: 1.2152 - val_accuracy: 0.6019\n",
      "Epoch 12/16\n",
      "50000/50000 [==============================] - 7s 144us/sample - loss: 0.7928 - accuracy: 0.7175 - val_loss: 1.1948 - val_accuracy: 0.5969\n",
      "Epoch 13/16\n",
      "50000/50000 [==============================] - 7s 142us/sample - loss: 0.7639 - accuracy: 0.7289 - val_loss: 1.2360 - val_accuracy: 0.5915\n",
      "Epoch 14/16\n",
      "50000/50000 [==============================] - 7s 144us/sample - loss: 0.7388 - accuracy: 0.7365 - val_loss: 1.2080 - val_accuracy: 0.6038\n",
      "Epoch 15/16\n",
      "50000/50000 [==============================] - 7s 144us/sample - loss: 0.7115 - accuracy: 0.7451 - val_loss: 1.2392 - val_accuracy: 0.6003\n",
      "Epoch 16/16\n",
      "50000/50000 [==============================] - 7s 141us/sample - loss: 0.6857 - accuracy: 0.7577 - val_loss: 1.2616 - val_accuracy: 0.6085\n"
     ]
    }
   ],
   "source": [
    "def create_baseline_model():\n",
    "  return tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(filters=6, kernel_size=5, activation='relu', input_shape=(32, 32, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(filters=16, kernel_size=5, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(120, activation='relu'),\n",
    "    tf.keras.layers.Dense(84, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')])\n",
    "\n",
    "def train_baseline_model():\n",
    "  model = create_baseline_model()\n",
    "  model.compile(optimizer=OPTIMIZER,\n",
    "                loss=LOSS,\n",
    "                metrics=[METRICS])\n",
    "\n",
    "  logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "  tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "  model.fit(x=x_train, \n",
    "            y=y_train, \n",
    "            epochs=EPOCHS, \n",
    "            validation_data=(x_test, y_test),\n",
    "            callbacks=[tensorboard_callback])\n",
    "\n",
    "%reload_ext tensorboard\n",
    "train_baseline_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "source": [
    "### Add L2 weight decay regularization: *Add an L2-norm penalty on the weights of your baseline model as regularization. Test two different regularization strengths.*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "UsageError: Line magic function `%rm` not found.\n"
     ]
    }
   ],
   "source": [
    "def create_L2_model(reg_strength):\n",
    "  return tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters=6, \n",
    "        kernel_size=5, \n",
    "        activation='relu', \n",
    "        input_shape=(32, 32, 3), \n",
    "        kernel_regularizer=tf.keras.regularizers.l2(reg_strength)),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters=16,\n",
    "        kernel_size=5, \n",
    "        activation='relu',\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(reg_strength)),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(\n",
    "        120,\n",
    "        activation='relu',\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(reg_strength)),\n",
    "    tf.keras.layers.Dense(\n",
    "        84,\n",
    "        activation='relu',\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(reg_strength)),\n",
    "    tf.keras.layers.Dense(10,\n",
    "        activation='softmax',\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(reg_strength))])\n",
    "\n",
    "def train_L2_model(reg_strength):\n",
    "  model = create_L2_model(reg_strength)\n",
    "  model.compile(optimizer=OPTIMIZER,\n",
    "                loss=LOSS,\n",
    "                metrics=[METRICS])\n",
    "\n",
    "  logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "  tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "  model.fit(x=x_train, \n",
    "            y=y_train, \n",
    "            epochs=EPOCHS, \n",
    "            validation_data=(x_test, y_test),\n",
    "            callbacks=[tensorboard_callback])\n",
    "\n",
    "\n",
    "train_L2_model(0.1)\n",
    "train_L2_model(0.01)"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Add L1 weight decay regularization: *Add an L1-norm penalty on the weights of your baseline model as regularization. Test two different regularization strengths.*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Remove fully-connected layers: *Modify the architecture to remove the fully-connected layers at the backend of the network. For example, replace with Global Average Pooling or an alternative. Report the change in the number of parameters for this model compared to previous.*\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "### Analyze the accuracy of the different models: *For all six models, train/test your model three times to get a sense of the consistency of the test error. Keep other aspects of your model the same among designs (# epochs, mini-batch size, hyperparameters). Generate a table that summarizes the training error, test error, standard deviation of test error across three runs, inference time, and \\# of parameters for each model.*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "### Analyze the weights of the regularized models: *For the baseline model and the four regularized models (from parts 2 and 3: L2 and L1 regularization with two different strengths each) measure the sparsity of the weights in each FC layer and create a bar chart that compares the sparsity between the models in each layer. There are a number of metrics that measure sparsity, e.g. Hoyer's index. For di\u000berent sparsity metrics, see Table I in https://arxiv.org/abs/0811.4706*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}